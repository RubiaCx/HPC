{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream K算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验\n",
    "\n",
    "### 实验设置\n",
    "\n",
    "- 修改内核模块配置：`(echo 'options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"') | sudo tee -a /etc/modprobe.d/RestrictedProfiling.conf >/dev/null` 将配置选项添加到系统的模块配置文件中，使得所有用户而非仅限管理员能够进行 NVIDIA GPU 的性能分析\n",
    "\n",
    "- 检查 NVIDIA 驱动的配置参数：`cat /proc/driver/nvidia/params | grep RmProfilingAdminOnly` 用来查看 NVIDIA 驱动的当前配置参数，验证前面修改的配置是否已生效\n",
    "\n",
    "- 设置复现实验的设置\n",
    "    - 启用 GPU 持久模式，使 GPU 在没有运行计算任务时不会关闭其电源状态，有助于避免启动计算时的延迟：`sudo nvidia-smi -pm 1 -i 0`\n",
    "    - 设置 GPU 功率限制，确保 GPU 不会超过设定的功率使用阈值：`sudo nvidia-smi -i 0 -pl 350  # 400 for A100`\n",
    "    - 锁定 GPU 时钟频率，提高运行实验的可重复性：`sudo nvidia-smi -i 0 -lgc 1005`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (echo 'options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"') | sudo tee -a /etc/modprobe.d/RestrictedProfiling.conf >/dev/null\n",
    "# sudo update-initramfs -u -k all\n",
    "# cat /proc/driver/nvidia/params | grep RmProfilingAdminOnly\n",
    "# sudo apt-get install zlib1g-dev\n",
    "# for reproductible experiments\n",
    "# sudo nvidia-smi -pm 1 -i 0\n",
    "# sudo nvidia-smi -i 0 -pl 350  # 400 for A100\n",
    "# sudo nvidia-smi -i 0 -lgc 1005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "# from triton.runtime.driver import CudaUtils # V100上 ImportError: cannot import name 'CudaUtils' from 'triton.runtime.driver'\n",
    "\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total SMs: 84\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2024)\n",
    "random.seed(2024)\n",
    "\n",
    "device = torch.cuda.current_device()\n",
    "# cuda_utils = CudaUtils()\n",
    "# total_sm = cuda_utils.get_device_properties(device)[\"multiprocessor_count\"]\n",
    "total_sm = 84\n",
    "print(f\"total SMs: {total_sm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triton kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 映射方式\n",
    "将当前计算单元的 ID tile_id 重新映射为 (pid_m, pid_n)\n",
    "\n",
    "`swizzle_tile`：通过改变矩阵块（tile）的访问顺序来优化数据的 L2 缓存利用\n",
    "\n",
    "- 将矩阵分块（BLOCK），进一步将矩阵块分组（GROUP），并在每个组内按特定的顺序访问块，以确保数据在被处理时尽可能地留在 L2 缓存中\n",
    "\n",
    "    - width = GROUP_M * grid_n：一个组内包含的总块数\n",
    "    - group_id = tile_id // width\n",
    "    - group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n",
    "    - pid_m = group_id * GROUP_M + (tile_id % group_size)\n",
    "    - pid_n = (tile_id % width) // group_size  \n",
    "\n",
    "`linear_tile`：简单线性映射\n",
    "    \n",
    "- 直接根据 `tile_id` 确定其在矩阵中的行和列位置，线性映射，从左到右，从上到下\n",
    "    - pid_m = tile_id // grid_n\n",
    "    - pid_n = tile_id % grid_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit()\n",
    "def swizzle_tile(tile_id,\n",
    "                 M, N, K,\n",
    "                 BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "                 GROUP_M: tl.constexpr): # 重排序时，每组包含的行块数量\n",
    "    # 计算矩阵在 M 和 N 方向上的块数，向下取整\n",
    "    grid_m = tl.cdiv(M, BLOCK_M)\n",
    "    grid_n = tl.cdiv(N, BLOCK_N)\n",
    "    # 确定重新排序的宽度\n",
    "    width = GROUP_M * grid_n\n",
    "    # 计算组 ID 和 组内尺寸\n",
    "    group_id = tile_id // width\n",
    "    group_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M) \n",
    "    # 确定行块和列块的位置\n",
    "    pid_m = group_id * GROUP_M + (tile_id % group_size)\n",
    "    pid_n = (tile_id % width) // group_size\n",
    "    return pid_m, pid_n\n",
    "\n",
    "@triton.jit()\n",
    "def linear_tile(tile_id,\n",
    "                M, N, K,\n",
    "                BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "                GROUP_M: tl.constexpr):\n",
    "    pid_m = tile_id // tl.cdiv(N, BLOCK_N)\n",
    "    pid_n = tile_id %  tl.cdiv(N, BLOCK_N)\n",
    "    return pid_m, pid_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一部分 n-tile Stream-K\n",
    "\n",
    "##### mac_loop()\n",
    "\n",
    "- 使用Triton实现基于tile和iter的矩阵乘法\n",
    "\n",
    "- 由于tiles可能被切分给不同的SMs，使用locks数组追踪每个 tile 的处理状态，保证数据一致性\n",
    "    - 初始状态为 0\n",
    "    - 如果当前迭代是一个 tile 的最后一次迭代，用 `tl.atomic_add` 将累加结果写回矩阵 C\n",
    "    - 用 `tl.atomic_xchg` 将锁状态更新为 1，表示该 tile 已经被完整处理\n",
    "    - 如果当前 tile 已经被部分处理，使用 `tl.atomic_cas` 确保其他计算单元在等待当前计算单元完成写入后再进行写入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit()\n",
    "def mac_loop(A, B, C,\n",
    "             M, N, K,\n",
    "             locks, # 用于在不同 GPU 线程之间同步的 locks 数组\n",
    "             stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, # 各个矩阵维度的步长\n",
    "             iters_per_tile, # 一个 tile 中的迭代次数\n",
    "             start_iter, end_iter,\n",
    "             BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "             ACC_TYPE: tl.constexpr, # 用于计算的累加器类型，保证足够的数值精度\n",
    "             GROUP_M: tl.constexpr):\n",
    "    # 确定当前块的位置\n",
    "    tile_id = start_iter // iters_per_tile\n",
    "    if GROUP_M > 0:\n",
    "        pid_m, pid_n = swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n",
    "    else:\n",
    "        pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n",
    "    # 计算内存地址\n",
    "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    rk = tl.arange(0, BLOCK_K)\n",
    "    A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak) + BLOCK_K * stride_ak * (start_iter % iters_per_tile)\n",
    "    B = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn) + BLOCK_K * stride_bk * (start_iter % iters_per_tile)\n",
    "    # 初始化 BLOCK_M x BLOCK_N 的累加器\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n",
    "    # 执行矩阵乘法\n",
    "    for current_iter in range(start_iter, end_iter):\n",
    "        a = tl.load(A)\n",
    "        b = tl.load(B)\n",
    "        acc += tl.dot(a, b)\n",
    "        A += BLOCK_K * stride_ak\n",
    "        B += BLOCK_K * stride_bk\n",
    "    # 写回结果\n",
    "    # 最后一次迭代处理当前块，则将累加器中的结果写回到矩阵 C 的相应位置\n",
    "    if end_iter % iters_per_tile == 0: # last iteration of the tile always happens before its start on another SM\n",
    "        C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)  # compute inside the if/else to avoid spilling!\n",
    "        tl.store(C_, acc)\n",
    "        if start_iter % iters_per_tile != 0:  # only if tile has been partially processed\n",
    "            tl.atomic_xchg(locks + tile_id, 1)\n",
    "    # 块已经被部分处理，使用原子交换（tl.atomic_xchg）来更新锁状态，确保结果的一致性\n",
    "    else:\n",
    "        while tl.atomic_cas(locks + tile_id, 1, 1) != 1:\n",
    "            pass\n",
    "        C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)  # compute inside the if/else to avoid spilling!\n",
    "        tl.atomic_add(C_, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first_wave()\n",
    "- 调用 `mac_loop()`处理分配不均的前几个tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit()\n",
    "def first_wave(\n",
    "        A, B, C,\n",
    "        M, N, K,\n",
    "        locks,\n",
    "        stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n",
    "        total_full_tiles_streamk, total_partial_tiles_streamk, \n",
    "        iters_per_tile,\n",
    "        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr,\n",
    "        GROUP_M: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "    start_iter = pid * total_full_tiles_streamk + tl.minimum(pid, total_partial_tiles_streamk)\n",
    "    last_iter = (pid + 1) * total_full_tiles_streamk + tl.minimum(pid + 1, total_partial_tiles_streamk)\n",
    "\n",
    "    while start_iter < last_iter:\n",
    "        end_iter = tl.minimum(start_iter + (iters_per_tile - start_iter % iters_per_tile), last_iter)\n",
    "        mac_loop(A, B, C,\n",
    "                 M, N, K,\n",
    "                 locks,\n",
    "                 stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n",
    "                 iters_per_tile,\n",
    "                 start_iter, end_iter,\n",
    "                 BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE,\n",
    "                 GROUP_M,)\n",
    "\n",
    "        start_iter = end_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二部分 Data-Parallel\n",
    "##### full_tiles()\n",
    "- 使用Triton实现处理完整 tiles 的矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit()\n",
    "def full_tiles(\n",
    "        A, B, C,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n",
    "        total_tiles_streamk,\n",
    "        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr,\n",
    "        GROUP_M: tl.constexpr,\n",
    "):\n",
    "    # 确定当前块的位置，total_tiles_streamk 是由 first_wave 处理的 tile 数量\n",
    "    tile_id = tl.program_id(0) + total_tiles_streamk\n",
    "    if GROUP_M > 0:\n",
    "        pid_m, pid_n = swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n",
    "    else:\n",
    "        pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\n",
    "    # 计算内存地址\n",
    "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    rk = tl.arange(0, BLOCK_K)\n",
    "    A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak)\n",
    "    B = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn)\n",
    "    # 初始化 BLOCK_M x BLOCK_N 的累加器\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n",
    "    # 执行矩阵乘法\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_K)):\n",
    "        a = tl.load(A)\n",
    "        b = tl.load(B)\n",
    "        acc += tl.dot(a, b)\n",
    "        A += BLOCK_K * stride_ak\n",
    "        B += BLOCK_K * stride_bk\n",
    "    acc = acc.to(tl.float16)  # 将累加器 acc 的数据类型转换为 tl.float16，以匹配输出矩阵 C 的数据类型\n",
    "    # 重新计算 rm 和 rn 以节省寄存器\n",
    "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n",
    "    tl.store(C, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMM Wrapper\n",
    "- two_tiles 模式表示每个 Stream-K 任务处理两个 tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matmul(torch.autograd.Function):\n",
    "    _debug = False\n",
    "\n",
    "    @staticmethod\n",
    "    def set_debug(debug: bool):\n",
    "        matmul._debug = debug\n",
    "\n",
    "    @staticmethod\n",
    "    def _call(a: torch.Tensor, b: torch.Tensor, \n",
    "              total_programs_streamk: int, \n",
    "              BLK_M: int, BLK_N: int, BLK_K: int, \n",
    "              two_tiles: bool, num_stages: int, num_warps: int):\n",
    "        device = a.device\n",
    "\n",
    "        # 确保输入的矩阵是连续存储的\n",
    "        assert a.is_contiguous() and b.is_contiguous(), \"输入的矩阵必须是连续的\"\n",
    "        assert a.shape[1] == b.shape[0], \"矩阵维度不匹配\"\n",
    "        M, K = a.shape\n",
    "        _, N = b.shape\n",
    "\n",
    "        # 根据输入矩阵的数据类型选择累加器类型\n",
    "        ACC_TYPE = tl.float32 if a.dtype in [torch.float16, torch.bfloat16, torch.float32] else tl.int32\n",
    "\n",
    "        # 计算需要的块数和迭代次数\n",
    "        total_blocks_M = triton.cdiv(M, BLK_M)\n",
    "        total_blocks_N = triton.cdiv(N, BLK_N)\n",
    "        iters_per_tile = triton.cdiv(K, BLK_K)\n",
    "        GROUP_M = 8  # 设置分组大小，0表示不使用 swizzle 而使用 linear\n",
    "        total_tiles = total_blocks_M * total_blocks_N\n",
    "\n",
    "        # 根据 total_programs_streamk 设置处理模式，计算 tiles 分配\n",
    "        if total_programs_streamk > 0:  # total_sm 或 n 倍的 total_sm\n",
    "            total_tiles_streamk = total_tiles % total_programs_streamk  # 计算 Stream-K 模式下的 tile 数量（除不尽的尾数）\n",
    "            if two_tiles and total_tiles - total_tiles_streamk > total_programs_streamk:  # 有小尾巴而且开启了 two_tiles\n",
    "                total_tiles_streamk += total_programs_streamk  # 增加 Stream-K 模式下处理的 tiles 数量，使得每个计算单元处理更多的 tiles\n",
    "            total_blocking_tiles = total_tiles - total_tiles_streamk\n",
    "            total_iters_streamk = total_tiles_streamk * iters_per_tile\n",
    "            total_full_tiles_streamk = total_iters_streamk // total_programs_streamk\n",
    "            total_partial_tiles_streamk = total_iters_streamk % total_programs_streamk\n",
    "        else: \n",
    "            total_blocking_tiles = total_tiles\n",
    "            total_tiles_streamk = 0\n",
    "            total_iters_streamk = 0\n",
    "            total_full_tiles_streamk = 0\n",
    "            total_partial_tiles_streamk = 0\n",
    "\n",
    "        # 输出调试信息（如果启用调试模式）\n",
    "        if matmul._debug:\n",
    "            print(f\"矩阵尺寸: M = {M}, N = {N}, K = {K}\")\n",
    "            print(f\"块尺寸: BLK_M = {BLK_M}, BLK_N = {BLK_N}, BLK_K = {BLK_K}\")\n",
    "            print(f\"总块数: {total_blocks_M} x {total_blocks_N} = {total_tiles}\")\n",
    "            print(f\"first_wave（Stream-K）的 tile 数量: {total_tiles_streamk}\")\n",
    "            print(f\"full_tiles（Data-Parallel） 的 tile 数量: {total_blocking_tiles}\")\n",
    "            print(f\"总 tiles 数量: {total_tiles_streamk} + {total_blocking_tiles} = {total_tiles}\")\n",
    "            print(f\"{total_programs_streamk=}\")\n",
    "            print(f\"{total_blocking_tiles=}\")\n",
    "            print(f\"{iters_per_tile=}\")\n",
    "            print(f\"{total_iters_streamk=}\")\n",
    "\n",
    "        # 分配输出矩阵 c 和用于同步的锁数组 locks\n",
    "        c = torch.empty((M, N), device=device, dtype=a.dtype)\n",
    "        locks = torch.zeros((total_tiles_streamk,), device=device, dtype=torch.int32)  # 分配锁数组，用于同步 SMs 之间的工作\n",
    "\n",
    "        # 调用 first_wave 函数处理第一波 tiles\n",
    "        k1 = first_wave[(total_programs_streamk,)](  # 启动 total_programs_streamk 个计算单元\n",
    "            a, b, c, M, N, K, locks,\n",
    "            a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n",
    "            total_full_tiles_streamk=total_full_tiles_streamk,\n",
    "            total_partial_tiles_streamk=total_partial_tiles_streamk,\n",
    "            iters_per_tile=iters_per_tile,\n",
    "            BLOCK_M=BLK_M, BLOCK_N=BLK_N, BLOCK_K=BLK_K,\n",
    "            ACC_TYPE=ACC_TYPE,\n",
    "            GROUP_M=GROUP_M,\n",
    "            num_stages=num_stages,\n",
    "            num_warps=num_warps)\n",
    "\n",
    "        if matmul._debug:\n",
    "            print(f\"first_wave：{k1.n_regs} registers used, {k1.n_spills} spills\")\n",
    "\n",
    "        # 调用 full_tiles 函数处理剩余的 tiles\n",
    "        k2 = full_tiles[(total_blocking_tiles,)](\n",
    "            a, b, c,\n",
    "            M, N, K,\n",
    "            a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n",
    "            total_tiles_streamk=total_tiles_streamk,\n",
    "            BLOCK_M=BLK_M, BLOCK_N=BLK_N, BLOCK_K=BLK_K,\n",
    "            ACC_TYPE=ACC_TYPE,\n",
    "            GROUP_M=GROUP_M,\n",
    "            num_stages=num_stages,\n",
    "            num_warps=num_warps)\n",
    "\n",
    "        # 输出 full_tiles 的调试信息\n",
    "        if matmul._debug:\n",
    "            print(f\"full_tiles：{k2.n_regs} registers used, {k2.n_spills} spills\")\n",
    "\n",
    "        return c\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx,  # ctx 是 PyTorch Autograd 引擎上下文，用于存储反向传播所需的信息\n",
    "                a: torch.Tensor, b: torch.Tensor, grid: int, \n",
    "                BLK_M=128, BLK_N=128, BLK_K=32, two_tiles=True, num_stages=3, num_warps=4):\n",
    "        return matmul._call(a=a, b=b, total_programs_streamk=grid, \n",
    "                            BLK_M=BLK_M, BLK_N=BLK_N, BLK_K=BLK_K, \n",
    "                            two_tiles=two_tiles, num_warps=num_warps, num_stages=num_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试\n",
    "\n",
    "#### 单样例测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵尺寸: M = 1536, N = 1792, K = 32000\n",
      "块尺寸: BLK_M = 128, BLK_N = 128, BLK_K = 32\n",
      "总块数: 12 x 14 = 168\n",
      "first_wave（Stream-K）的 tile 数量: 84\n",
      "full_tiles（Data-Parallel） 的 tile 数量: 84\n",
      "总 tiles 数量: 84 + 84 = 168\n",
      "total_programs_streamk=84\n",
      "total_blocking_tiles=84\n",
      "iters_per_tile=1000\n",
      "total_iters_streamk=84000\n",
      "first_wave：234 registers used, 0 spills\n",
      "full_tiles：218 registers used, 0 spills\n",
      "PyTorch 矩阵乘法耗时: 2.18 ms\n",
      "Hybrid stream-k 模式 (grid=84) 耗时: 4.65 ms, 加速比: 0.47\n",
      "Hybrid stream-k 模式 (grid=168) 耗时: 4.17 ms, 加速比: 0.52\n",
      "Tile 矩阵乘法 (grid=0) 耗时: 4.11 ms, 加速比: 0.53\n"
     ]
    }
   ],
   "source": [
    "# m, n, k = 128, 128, 6400 # some problem size to test\n",
    "# m, n, k = 1536, 1792, 6016  # some problem size to test\n",
    "m, n, k = 1536, 1792, 32000  # some problem size to test\n",
    "A = torch.randn(m, k, device=\"cuda\", dtype=torch.float16)\n",
    "B = torch.randn(k, n, device=\"cuda\", dtype=torch.float16)\n",
    "total_sm = 84\n",
    "\n",
    "matmul.set_debug(True)\n",
    "C = matmul.apply(A, B, total_sm, 128, 128, 32, 4, 4)\n",
    "matmul.set_debug(False)\n",
    "# 使用 PyTorch 的矩阵乘法运算符 @ 计算期望的结果矩阵 expected\n",
    "expected = A @ B\n",
    "assert torch.allclose(C, expected, atol=1), f\"max: {(C - expected).abs().max().item()}\\n{C}\\n{expected}\"\n",
    "\n",
    "# for debugging, uncomment the following line\n",
    "# exit(0)\n",
    "\n",
    "pytorch_ms = triton.testing.do_bench(lambda: torch.matmul(A, B))\n",
    "print(f\"PyTorch 矩阵乘法耗时: {pytorch_ms:.2f} ms\")\n",
    "\n",
    "triton_ms = triton.testing.do_bench(lambda: matmul.apply(A, B, total_sm, 128, 128, 32, True, 4, 4))\n",
    "speedup = pytorch_ms / triton_ms\n",
    "print(f\"Hybrid stream-k 模式 (grid={total_sm}) 耗时: {triton_ms:.2f} ms, 加速比: {speedup:.2f}\")\n",
    "\n",
    "triton_ms = triton.testing.do_bench(lambda: matmul.apply(A, B, total_sm * 2, 128, 128, 32, True, 4, 4))\n",
    "speedup = pytorch_ms / triton_ms\n",
    "print(f\"Hybrid stream-k 模式 (grid={total_sm * 2}) 耗时: {triton_ms:.2f} ms, 加速比: {speedup:.2f}\")\n",
    "\n",
    "triton_ms = triton.testing.do_bench(lambda: matmul.apply(A, B, 0, 128, 128, 32, True, 4, 4))\n",
    "speedup = pytorch_ms / triton_ms\n",
    "print(f\"Tile 矩阵乘法 (grid=0) 耗时: {triton_ms:.2f} ms, 加速比: {speedup:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "矩阵尺寸: M = 1536, N = 1792, K = 6016\n",
      "块尺寸: BLK_M = 128, BLK_N = 128, BLK_K = 32\n",
      "总块数: 12 x 14 = 168\n",
      "first_wave（Stream-K）的 tile 数量: 86\n",
      "full_tiles（Data-Parallel） 的 tile 数量: 82\n",
      "总 tiles 数量: 86 + 82 = 168\n",
      "total_programs_streamk=82\n",
      "total_blocking_tiles=82\n",
      "iters_per_tile=188\n",
      "total_iters_streamk=16168\n",
      "first_wave：234 registers used, 0 spills\n",
      "full_tiles：218 registers used, 0 spills\n",
      "PyTorch 矩阵乘法耗时: 0.46 ms\n",
      "Hybrid stream-k 模式 (grid=82) 耗时: 0.95 ms, 加速比: 0.49\n",
      "Hybrid stream-k 模式 (grid=164) 耗时: 0.80 ms, 加速比: 0.58\n",
      "Tile 矩阵乘法 (grid=0) 耗时: 0.75 ms, 加速比: 0.62\n"
     ]
    }
   ],
   "source": [
    "# 定义矩阵尺寸\n",
    "m, n, k = 1536, 1792, 6016  # 用于测试的问题规模\n",
    "total_sm = 82\n",
    "\n",
    "# 在 GPU 上生成随机矩阵 A 和 B\n",
    "A = torch.randn(m, k, device=\"cuda\", dtype=torch.float16)\n",
    "B = torch.randn(k, n, device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "# 启用调试模式并执行矩阵乘法\n",
    "matmul.set_debug(True)\n",
    "C = matmul.apply(A, B, total_sm, 128, 128, 32, 4, 4)\n",
    "matmul.set_debug(False)\n",
    "\n",
    "# 使用 PyTorch 的矩阵乘法运算符 @ 计算期望的结果矩阵 expected\n",
    "expected = A @ B\n",
    "\n",
    "assert torch.allclose(C, expected, atol=1), f\"最大误差: {(C - expected).abs().max().item()}\\n计算结果:\\n{C}\\n期望结果:\\n{expected}\"\n",
    "\n",
    "pytorch_ms = triton.testing.do_bench(lambda: torch.matmul(A, B))\n",
    "print(f\"PyTorch 矩阵乘法耗时: {pytorch_ms:.2f} ms\")\n",
    "\n",
    "triton_ms = triton.testing.do_bench(lambda: matmul.apply(A, B, total_sm, 128, 128, 32, True, 4, 4))\n",
    "speedup = pytorch_ms / triton_ms\n",
    "print(f\"Hybrid stream-k 模式 (grid={total_sm}) 耗时: {triton_ms:.2f} ms, 加速比: {speedup:.2f}\")\n",
    "\n",
    "triton_ms = triton.testing.do_bench(lambda: matmul.apply(A, B, total_sm * 2, 128, 128, 32, True, 4, 4))\n",
    "speedup = pytorch_ms / triton_ms\n",
    "print(f\"Hybrid stream-k 模式 (grid={total_sm * 2}) 耗时: {triton_ms:.2f} ms, 加速比: {speedup:.2f}\")\n",
    "\n",
    "triton_ms = triton.testing.do_bench(lambda: matmul.apply(A, B, 0, 128, 128, 32, True, 4, 4))\n",
    "speedup = pytorch_ms / triton_ms\n",
    "print(f\"Tile 矩阵乘法 (grid=0) 耗时: {triton_ms:.2f} ms, 加速比: {speedup:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多样例测试\n",
    "- 随机生成测试矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000  # 32768 # 生成的样本数量\n",
    "step = 256                  # 样本尺寸的步长\n",
    "# 使用 torch.logspace 生成从 step 到 8192 的对数刻度值，然后通过四舍五入和去重生成唯一的尺寸值\n",
    "values = ((torch.logspace(torch.tensor(step).log2(), torch.tensor(8192).log2(), num_samples, base=2) / step).round() * step).unique().tolist()\n",
    "# 通过尺寸值生成所有可能的(m, n, k) 并从中随机抽取 num_samples 个组合\n",
    "shapes = [(int(m), int(n), int(k)) for m in values for n in values for k in values]\n",
    "shapes = random.sample(shapes, num_samples)\n",
    "assert len(shapes) == num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/1000 - average speedup: 0.881\n",
      "20/1000 - average speedup: 0.893\n",
      "30/1000 - average speedup: 0.951\n",
      "40/1000 - average speedup: 0.960\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# baseline: pytorch\u001b[39;00m\n\u001b[1;32m     18\u001b[0m expected \u001b[38;5;241m=\u001b[39m A \u001b[38;5;241m@\u001b[39m B\n\u001b[0;32m---> 19\u001b[0m pytorch_ms \u001b[38;5;241m=\u001b[39m \u001b[43mtriton\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# test: triton\u001b[39;00m\n\u001b[1;32m     21\u001b[0m measures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/triton/testing.py:148\u001b[0m, in \u001b[0;36mdo_bench\u001b[0;34m(fn, warmup, rep, grad_to_none, quantiles, fast_flush, return_mode)\u001b[0m\n\u001b[1;32m    146\u001b[0m     end_event[i]\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Record clocks\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m times \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([s\u001b[38;5;241m.\u001b[39melapsed_time(e) \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(start_event, end_event)], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quantiles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.8/site-packages/torch/cuda/__init__.py:792\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    790\u001b[0m _lazy_init()\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for idx, (m, n, k) in enumerate(shapes):\n",
    "    # 每处理 10 个样本，输出当前进度和平均加速比\n",
    "    if idx % 10 == 0 and idx > 0:\n",
    "        speedups = [r[\"speedup\"] for r in results]\n",
    "        print(f\"{idx}/{num_samples} - average speedup: {sum(speedups) / len(speedups):.3f}\")\n",
    "\n",
    "    A = torch.randn(m, k, device=\"cuda\", dtype=torch.float16)\n",
    "    B = torch.randn(k, n, device=\"cuda\", dtype=torch.float16)\n",
    "    output: Optional[torch.Tensor] = None\n",
    "\n",
    "\n",
    "    def wrapper_matmul(*args, **kwargs):\n",
    "        global output\n",
    "        output = matmul.apply(*args, **kwargs)\n",
    "        return output\n",
    "    # baseline: pytorch\n",
    "    expected = A @ B\n",
    "    pytorch_ms = triton.testing.do_bench(lambda: A @ B)\n",
    "    # test: triton\n",
    "    measures = list()\n",
    "    for two_tiles in [True, False]:\n",
    "        nb_sm = [total_sm, total_sm * 2]\n",
    "        total_tile = (m // 128) * (n // 128)\n",
    "        if total_tile < total_sm * 2:\n",
    "            nb_sm.append(total_tile)\n",
    "        nb_sm += random.sample(range(2, total_sm * 2, 2), 10)\n",
    "        for sm in nb_sm:\n",
    "            triton_ms = triton.testing.do_bench(lambda: wrapper_matmul(A, B, sm, 128, 128, 32, two_tiles, 4, 4))\n",
    "            max_disc = (output - expected).abs().max().item()\n",
    "            # large tolerance to accomodate for large K (rounding due to half precision), we just want to catch bugs.\n",
    "            assert max_disc <= 5., f\"pb size: {m}x{n}x{k} - max discrepancy: {max_disc} - sm: {sm}, 2 tiles: {two_tiles}\\n{output}\\n{expected}\"\n",
    "            info = {\n",
    "                \"2 tiles\": two_tiles,\n",
    "                \"sm\": sm,\n",
    "                \"disc\": max_disc,\n",
    "                \"triton_ms\": triton_ms,\n",
    "            }\n",
    "            measures.append(info)\n",
    "    best_triton_ms = min([m[\"triton_ms\"] for m in measures])\n",
    "    d = {\"m\": m, \"n\": n, \"k\": k,\n",
    "         \"triton\": measures,\n",
    "         \"pytorch_ms\": pytorch_ms,\n",
    "         \"speedup\": pytorch_ms / best_triton_ms}\n",
    "    results.append(d)\n",
    "    measures = list()\n",
    "\n",
    "results.sort(key=lambda x: x[\"speedup\"], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32760/32768 - average speedup: 0.962 (A100)\n",
    "# 990/1000 - average speedup: 1.063 (3090 RTX with while loop and 2 tiles disabled / enabled)\n",
    "# 990/1000 - average speedup: 1.016（V100）\n",
    "# 990/32768 - average speedup: 0.608（A100）\n",
    "# 100/32768 - average speedup: 0.901"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 详解\n",
    "- 矩阵乘法 $C=A×B$，其中 $A$ 的大小为 $m×k$，$B$ 的大小为 $k×n$\n",
    "\n",
    "- $m=1536$，$n=1792$，$k=6016$\n",
    "\n",
    "- $BLOCK\\_M=128$，$BLOCK\\_N=128$，$BLOCK\\_K=32$\n",
    "\n",
    "- 需要处理的块数量：\n",
    "    - $ total\\_blocks\\_M = \\lceil \\frac{1536}{128} \\rceil = 12 $\n",
    "    - $ total\\_blocks\\_N = \\lceil \\frac{1792}{128} \\rceil = 14 $\n",
    "    - 总块数 $ total\\_tiles = total\\_blocks\\_M \\times total\\_blocks\\_N = 12 \\times 14 = 168 $\n",
    "- 每个块的迭代次数：$ iters\\_per\\_tile = \\lceil \\frac{6016}{32} \\rceil = 188 $\n",
    "\n",
    "#### $total\\_programs\\_streamk = 82$\n",
    "- 总共使用 82 个SM来进行并行计算\n",
    "\n",
    "- $ total\\_tiles\\_streamk = total\\_tiles \\% total\\_programs\\_streamk = 168 \\% 82 = 4 $，由`first_wave` 处理，每个SM处理9次完整迭代和第10个迭代中的部分（32次）任务\n",
    "    - $ total\\_full\\_tiles\\_streamk = \\frac{total\\_iters\\_streamk}{total\\_programs\\_streamk} = \\frac{4 \\times 188}{82} \\approx 9 $\n",
    "    - $ total\\_partial\\_tiles\\_streamk = (4 \\times 188) \\% 82 = 32 $\n",
    "- $total\\_blocking\\_tiles = total\\_tiles - total\\_tiles\\_streamk = 168 - 4 = 164$，由 `full_tiles` 处理\n",
    "\n",
    "#### $total\\_programs\\_streamk = 84（V100）$\n",
    "- 总共使用 84 个SM来进行并行计算\n",
    "\n",
    "- $ total\\_tiles\\_streamk = total\\_tiles \\% total\\_programs\\_streamk = 168 \\% 84 = 0 $\n",
    "- $total\\_blocking\\_tiles = total\\_tiles - total\\_tiles\\_streamk = 168 - 0 = 168$，所有的计算块都由 `full_tiles` 处理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
